{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5377440,"sourceType":"datasetVersion","datasetId":3119215},{"sourceId":5891144,"sourceType":"datasetVersion","datasetId":3345370},{"sourceId":6300428,"sourceType":"datasetVersion","datasetId":3624253},{"sourceId":10132950,"sourceType":"datasetVersion","datasetId":6253789},{"sourceId":10133346,"sourceType":"datasetVersion","datasetId":6254007},{"sourceId":10163687,"sourceType":"datasetVersion","datasetId":6276290}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/renesta/tiny-yolo?scriptVersionId=218810445\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import kagglehub\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn as nn\nfrom torchvision import transforms\nimport cv2\nimport torchvision\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport torch.nn.functional as F\nimport re\n\n# Download latest version\npath1 = kagglehub.dataset_download(\"iamtushara/face-detection-dataset\")\npath2 = kagglehub.dataset_download(\"fareselmenshawii/face-detection-dataset\")\n\nprint(\"Path to dataset1 files:\", path1)\nprint(\"Path to dataset2 files:\", path2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:25:46.403752Z","iopub.execute_input":"2025-01-20T01:25:46.404104Z","iopub.status.idle":"2025-01-20T01:25:46.795864Z","shell.execute_reply.started":"2025-01-20T01:25:46.404076Z","shell.execute_reply":"2025-01-20T01:25:46.794991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:25:46.797614Z","iopub.execute_input":"2025-01-20T01:25:46.798227Z","iopub.status.idle":"2025-01-20T01:25:46.802115Z","shell.execute_reply.started":"2025-01-20T01:25:46.798188Z","shell.execute_reply":"2025-01-20T01:25:46.801337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset25(Dataset):\n    def __init__(self, root_paths, grid_sizes=[13, 26, 52], YOLO_shape=(416, 416), transform=None):\n        self.root_paths = root_paths\n        self.images_folder = [os.path.join(root_paths[0], \"merged/images/train\"), os.path.join(root_paths[1], \"images/train\")]\n        self.labels_folder = [os.path.join(root_paths[0], \"merged/labels/train\"), os.path.join(root_paths[1], \"labels/train\")]\n        self.transform = transform\n        self.YOLO_shape = YOLO_shape\n        self.grid_sizes = grid_sizes\n        self.images = []\n        self.targets = []\n\n        for index, images_folder in enumerate(self.images_folder): \n\n            for filename in os.listdir(images_folder):\n                \n                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    image_path = os.path.join(images_folder, filename)\n                    label_path = os.path.join(self.labels_folder[index], filename.replace('.jpg', '.txt'))\n                    label_path1 = os.path.join(self.labels_folder[index], filename.replace('.jpeg', '.txt'))\n                    if os.path.exists(label_path):\n                        self.images.append(image_path)\n                        self.targets.append(label_path)\n                    elif os.path.exists(label_path1):\n                        self.images.append(image_path)\n                        self.targets.append(label_path1)\n                    else:\n                        print(f\"Warning: Label file not found for {filename}\")\n\n    def __len__(self):\n        return len(self.images) \n\n    def __getitem__(self, index):\n        \n        image_path = self.images[index]\n        label_path = self.targets[index]\n        pattern = re.compile(r'.*iamtushara.*')\n        match = pattern.search(image_path)\n        match = 1\n\n        img = Image.open(image_path).convert(\"RGB\")\n\n\n        with open(label_path, 'r') as f:\n            lines = f.readlines()\n\n        annotations = []\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) < 4:\n                continue\n\n            x, y, width, height = parts[-4:]\n            x = float(x)\n            y = float(y)\n            width = float(width)\n            height = float(height)\n            annotations.append((x, y, width, height))\n\n\n        target_list = []\n        for grid_size in self.grid_sizes:\n            target = np.zeros((grid_size * grid_size, 5), dtype=np.float32)\n            img_width, img_height = img.size\n\n            for annot in annotations:\n                \n                x, y, width, height = annot\n                \n                x_center = ((x + width/2) * self.YOLO_shape[0] if match else (x) / img_width * self.YOLO_shape[0])\n                y_center = ((y + height/2) * self.YOLO_shape[1] if match else (y) / img_height * self.YOLO_shape[1])\n                box_width = (width * self.YOLO_shape[0] if match else width / img_width * self.YOLO_shape[0])\n                box_height = (height * self.YOLO_shape[1] if match else height / img_height * self.YOLO_shape[1])\n\n                grid_x = ((grid_size / self.YOLO_shape[0]) * x_center)\n                grid_y = ((grid_size / self.YOLO_shape[1]) * y_center)\n\n                target_idx = int(grid_y - 0.000001) * grid_size + int(grid_x - 0.000001)\n\n                target[target_idx, 0] = 1.0 \n                target[target_idx, 1] = (grid_x - int(grid_x)) * self.YOLO_shape[0] / grid_size\n                target[target_idx, 2] = (grid_y - int(grid_y)) * self.YOLO_shape[1] / grid_size\n                target[target_idx, 3] = box_width\n                target[target_idx, 4] = box_height\n\n            target_list.append(torch.from_numpy(target).float())\n\n        img = img.resize(self.YOLO_shape)\n        \n        if self.transform:\n            img = self.transform(img)\n\n        return img, target_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:25:46.803315Z","iopub.execute_input":"2025-01-20T01:25:46.803637Z","iopub.status.idle":"2025-01-20T01:25:46.945596Z","shell.execute_reply.started":"2025-01-20T01:25:46.803594Z","shell.execute_reply":"2025-01-20T01:25:46.944726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt = Dataset25([path1,path2], transform = transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:25:46.947599Z","iopub.execute_input":"2025-01-20T01:25:46.947902Z","iopub.status.idle":"2025-01-20T01:27:29.045971Z","shell.execute_reply.started":"2025-01-20T01:25:46.947877Z","shell.execute_reply":"2025-01-20T01:27:29.045099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data, validation_data = random_split(dt, [0.9, 0.1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.047511Z","iopub.execute_input":"2025-01-20T01:27:29.047895Z","iopub.status.idle":"2025-01-20T01:27:29.061945Z","shell.execute_reply.started":"2025-01-20T01:27:29.047857Z","shell.execute_reply":"2025-01-20T01:27:29.061165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size = 128, shuffle = True, pin_memory=True)\nvalidation_loader = DataLoader(validation_data, batch_size = 128, shuffle = True, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.062941Z","iopub.execute_input":"2025-01-20T01:27:29.063234Z","iopub.status.idle":"2025-01-20T01:27:29.068368Z","shell.execute_reply.started":"2025-01-20T01:27:29.063201Z","shell.execute_reply":"2025-01-20T01:27:29.067672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(dt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.069302Z","iopub.execute_input":"2025-01-20T01:27:29.069564Z","iopub.status.idle":"2025-01-20T01:27:29.080257Z","shell.execute_reply.started":"2025-01-20T01:27:29.069536Z","shell.execute_reply":"2025-01-20T01:27:29.079491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    \n    def __init__(self, patience=10, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    print(\"Early stopping triggered.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.081247Z","iopub.execute_input":"2025-01-20T01:27:29.081486Z","iopub.status.idle":"2025-01-20T01:27:29.089305Z","shell.execute_reply.started":"2025-01-20T01:27:29.08146Z","shell.execute_reply":"2025-01-20T01:27:29.088668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_box_to_best_scale(w, h, THRESH_LARGE, THRESH_MEDIUM):\n    \n    box_area = w * h\n    if box_area > THRESH_LARGE:\n        return 13\n    elif box_area > THRESH_MEDIUM:\n        return 26\n    else:\n        return 52","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.090083Z","iopub.execute_input":"2025-01-20T01:27:29.090355Z","iopub.status.idle":"2025-01-20T01:27:29.101944Z","shell.execute_reply.started":"2025-01-20T01:27:29.090323Z","shell.execute_reply":"2025-01-20T01:27:29.10113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    def __init__(self, lambda_l2=0.001, lambda_conf=5.0, lambda_box = 1.0, THRESH_MEDIUM = 3000, THRESH_LARGE = 10000):\n\n        super(YoloLoss, self).__init__()\n        self.lambda_l2 = lambda_l2\n        self.lambda_conf = lambda_conf\n        self.lambda_box = lambda_box\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.THRESH_MEDIUM = 3000\n        self.THRESH_LARGE = 10000\n\n    def assign_box_to_best_scale(self, w, h, THRESH_MEDIUM, THRESH_LARGE):\n        box_area = w * h\n        scale = torch.where(box_area > THRESH_LARGE, torch.tensor(13.0), torch.where(box_area > THRESH_MEDIUM, torch.tensor(26.0), torch.tensor(52.0)))\n        return scale\n\n    def compute_iou_vectorized(self, pred_boxes, target_boxes):\n       \n        xmin_intersect = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n        ymin_intersect = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n        xmax_intersect = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n        ymax_intersect = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n        \n        # Compute intersection area\n        width_intersect = torch.clamp(xmax_intersect - xmin_intersect, min=0)\n        height_intersect = torch.clamp(ymax_intersect - ymin_intersect, min=0)\n        area_intersect = width_intersect * height_intersect\n        \n        # Compute areas of prediction and target boxes\n        area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n        area_target = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n        \n        # Compute union area\n        area_union = area_pred + area_target - area_intersect\n        \n        # Compute IoU\n        iou = area_intersect / area_union\n        return iou\n\n    def forward(self, all_predictions, all_targets):\n\n        total_loss = 0\n        scales = [13, 26, 52]\n        \n        for predictions, targets, scale in zip(all_predictions, all_targets, scales):\n            \n            w = targets[..., 3]\n            h = targets[..., 4]\n\n            assigned_scale = self.assign_box_to_best_scale(w, h, self.THRESH_MEDIUM, self.THRESH_LARGE)\n            scale_mask = assigned_scale == scale\n            \n            object_mask = targets[..., 0] == 1\n            no_object_mask = targets[..., 0] == 0\n\n            object_mask_scale = object_mask & scale_mask\n    \n            pred_conf = torch.sigmoid(predictions[..., 0])\n            target_conf = targets[..., 0]\n    \n            conf_loss_obj = self.bce_loss(pred_conf[object_mask_scale], target_conf[object_mask_scale])\n            conf_loss_no_obj = self.bce_loss(pred_conf[no_object_mask], target_conf[no_object_mask])\n    \n            conf_loss_obj = conf_loss_obj.mean()\n            conf_loss_no_obj = conf_loss_no_obj.mean()\n    \n            pred_boxes = predictions[object_mask_scale]\n            target_boxes = targets[object_mask_scale]\n\n            box_loss = 0\n    \n            if pred_boxes.numel() > 0:\n\n                pred_x = pred_boxes[:, 1]\n                pred_y = pred_boxes[:, 2]\n                pred_w = pred_boxes[:, 3]\n                pred_h = pred_boxes[:, 4]\n                pred_xmin = pred_x - pred_w / 2\n                pred_ymin = pred_y - pred_h / 2\n                pred_xmax = pred_x + pred_w / 2\n                pred_ymax = pred_y + pred_h / 2\n                pred_boxes_corner = torch.stack([pred_xmin, pred_ymin, pred_xmax, pred_ymax], dim=1)\n                \n                target_x = target_boxes[:, 1]\n                target_y = target_boxes[:, 2]\n                target_w = target_boxes[:, 3]\n                target_h = target_boxes[:, 4]\n                target_xmin = target_x - target_w / 2\n                target_ymin = target_y - target_h / 2\n                target_xmax = target_x + target_w / 2\n                target_ymax = target_y + target_h / 2\n                target_boxes_corner = torch.stack([target_xmin, target_ymin, target_xmax, target_ymax], dim=1)\n                \n                # Compute IoU\n                iou = self.compute_iou_vectorized(pred_boxes_corner, target_boxes_corner)\n                # Compute box loss\n                box_loss = 1 - iou\n                box_loss = box_loss.mean()\n                    \n            else:\n                box_loss = torch.tensor(0.0, device=predictions.device)\n    \n            l2_reg = 0\n            for name, param in model.named_parameters():\n                if 'bias' not in name:\n                    l2_reg += torch.norm(param) ** 2\n    \n            l2_loss = self.lambda_l2 * l2_reg\n    \n            total_loss += (self.lambda_box * box_loss + self.lambda_conf * 2 * conf_loss_obj + self.lambda_conf * conf_loss_no_obj + l2_loss)\n\n        return total_loss, self.lambda_conf * (conf_loss_obj + conf_loss_no_obj), self.lambda_box * box_loss, l2_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:32:19.926313Z","iopub.execute_input":"2025-01-20T01:32:19.926732Z","iopub.status.idle":"2025-01-20T01:32:19.953944Z","shell.execute_reply.started":"2025-01-20T01:32:19.926692Z","shell.execute_reply":"2025-01-20T01:32:19.952671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef conv_bn_leaky(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n    \"\"\"Удобный блок: Conv2d -> BatchNorm -> LeakyReLU.\"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.1, inplace=True)\n    )\n\ndef conv1x1_bn_leaky(in_channels, out_channels):\n    \"\"\"Блок 1x1 сверка для сжатия/расширения каналов.\"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.1, inplace=True)\n    )\n\n# ---------------------------\n#    YOLO-like Backbone\n# ---------------------------\nclass TinyDarknetBackbone(nn.Module):\n    \"\"\"\n    Упрощённый backbone, напоминающий Tiny-YOLO/Darknet\n    \"\"\"\n    def __init__(self):\n        super(TinyDarknetBackbone, self).__init__()\n        \n        # (3 x 416 x 416) -> (16 x 208 x 208)\n        self.block1 = nn.Sequential(\n            conv_bn_leaky(3, 16, 3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2)  # downsample в 2 раза\n        )\n        \n        # (16 x 208 x 208) -> (32 x 104 x 104)\n        self.block2 = nn.Sequential(\n            conv_bn_leaky(16, 32, 3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # (32 x 104 x 104) -> (64 x 52 x 52)\n        self.block3 = nn.Sequential(\n            conv_bn_leaky(32, 64, 3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # (64 x 52 x 52) -> (128 x 26 x 26)\n        self.block4 = nn.Sequential(\n            conv_bn_leaky(64, 128, 3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # (128 x 26 x 26) -> (256 x 13 x 13)\n        self.block5 = nn.Sequential(\n            conv_bn_leaky(128, 256, 3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        # (256 x 13 x 13) -> здесь остановка (выход low-res)\n        self.block6 = nn.Sequential(\n            conv_bn_leaky(256, 512, 3, stride=1, padding=1)\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Возвращаем фичи с двух промежуточных этапов (для skip connection):\n         - medium_output (26x26)\n         - small_output  (13x13)\n        А также фичу (52x52) можно &laquo;снять&raquo; до 4-го maxpool’a, \n        если понадобится “более мелкая” голова.\n        \"\"\"\n        x = self.block1(x)  # (16 x 208 x 208)\n        x = self.block2(x)  # (32 x 104 x 104)\n        x = self.block3(x)  # (64 x 52 x 52)\n\n        feat_52 = x\n        \n        x = self.block4(x)  # (128 x 26 x 26)\n        feat_26 = x\n        \n        x = self.block5(x)  # (256 x 13 x 13)\n        x = self.block6(x)  # (512 x 13 x 13)\n        feat_13 = x\n        \n        return feat_13, feat_26, feat_52\n\nclass YoloHead(nn.Module):\n    \"\"\"\n    Упрощённо: свёрточные блоки, потом 1x1 Conv -> (out_channels).\n    \"\"\"\n    def __init__(self, in_channels_13=512, in_channels_26=128, in_channels_52=64, \n                 out_channels=5):\n        super(YoloHead, self).__init__()\n        \n        # Голова на 13x13\n        self.conv_13 = nn.Sequential(\n            conv_bn_leaky(in_channels_13, 256, 3, 1, 1),\n            nn.Conv2d(256, out_channels, 1, 1, 0)\n        )\n        \n        # Голова на 26x26\n        # Но сперва добавим блок, чтобы поднять (upsample) и слить 13x13 => 26x26\n\n        self.skip_13_to_26 = conv1x1_bn_leaky(in_channels_13, 128)\n        self.upsample_13 = nn.Upsample(scale_factor=2, mode='nearest')\n        \n        self.conv_26 = nn.Sequential(\n            conv_bn_leaky(in_channels_26 + 128, 128, 3, 1, 1),\n            nn.Conv2d(128, out_channels, 1, 1, 0)\n        )\n        \n        # Голова на 52x52\n        self.skip_26_to_52 = conv1x1_bn_leaky(in_channels_26 + 128, 64)\n        self.upsample_26 = nn.Upsample(scale_factor=2, mode='nearest')\n        \n        self.conv_52 = nn.Sequential(\n            conv_bn_leaky(in_channels_52 + 64, 64, 3, 1, 1),\n            nn.Conv2d(64, out_channels, 1, 1, 0)\n        )\n\n    def forward(self, feat_13, feat_26, feat_52):\n        \"\"\"\n        feat_13: (512, 13, 13)\n        feat_26: (128, 26, 26)\n        feat_52: (64, 52, 52)\n        \"\"\"\n        out_13 = self.conv_13(feat_13)  \n        # out_13 -> (N, out_channels, 13, 13)\n        \n        #    cначала берём feat_13, уменьшаем каналы до 128 и upsample x2 => (128, 26, 26)\n        up_13 = self.skip_13_to_26(feat_13) \n        up_13 = self.upsample_13(up_13)     # (128, 26, 26)\n        \n        #  конкатенируем с feat_26 => (128+128, 26, 26)\n        merge_26 = torch.cat([feat_26, up_13], dim=1)\n        out_26 = self.conv_26(merge_26)\n        \n        # для 52x52\n        up_26 = self.skip_26_to_52(merge_26) \n        up_26 = self.upsample_26(up_26)     # (64, 52, 52)\n        \n        # конкатенируем с feat_52 => (64+64, 52, 52)\n        merge_52 = torch.cat([feat_52, up_26], dim=1)\n        out_52 = self.conv_52(merge_52)\n        \n        # out_13: (N, 5, 13, 13) -> (N, 13*13, 5)\n        N = out_13.size(0)\n        \n        out_13 = out_13.permute(0, 2, 3, 1).contiguous().view(N, -1, 5)\n        out_26 = out_26.permute(0, 2, 3, 1).contiguous().view(N, -1, 5)\n        out_52 = out_52.permute(0, 2, 3, 1).contiguous().view(N, -1, 5)\n\n        return [out_13, out_26, out_52]\n\n\n# ---------------------------\n#    Итоговая модель\n# ---------------------------\nclass YoloModel(nn.Module):\n    def __init__(self, out_channels=5):\n        super(YoloModel, self).__init__()\n        self.backbone = TinyDarknetBackbone()\n        self.head = YoloHead(\n            in_channels_13=512,  # соответствует выходу backbone на 13x13\n            in_channels_26=128,\n            in_channels_52=64,\n            out_channels=out_channels\n        )\n        \n    def forward(self, x):\n        feat_13, feat_26, feat_52 = self.backbone(x)\n        outputs = self.head(feat_13, feat_26, feat_52)\n        return outputs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.128848Z","iopub.execute_input":"2025-01-20T01:27:29.12911Z","iopub.status.idle":"2025-01-20T01:27:29.155817Z","shell.execute_reply.started":"2025-01-20T01:27:29.129084Z","shell.execute_reply":"2025-01-20T01:27:29.155103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    \n    scaler = GradScaler()\n    \n    model.train()\n    total_loss = 0\n    total_conf_loss = 0\n    total_box_loss = 0\n    total_l2_loss = 0\n\n\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n\n        inputs = inputs.to(device)\n        targets = [t.to(device) for t in targets]\n\n        optimizer.zero_grad()\n\n        with autocast():\n            \n            outputs = model(inputs)\n            loss, conf_loss, box_loss, l2_loss = criterion(outputs, targets)\n\n        loss.backward()\n\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_conf_loss += conf_loss.item()\n        total_box_loss += box_loss.item()\n        total_l2_loss += l2_loss.item()\n\n        if batch_idx % 12 == 0:\n            print(f'Batch {batch_idx}, Total Loss: {loss.item():.4f}, '\n                  f'Confidence Loss: {conf_loss.item():.4f}, '\n                  f'Box Loss: {box_loss.item():.4f}, '\n                  f'L2 Loss: {l2_loss.item():.4f}')\n\n\n    return total_loss / len(dataloader)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.156518Z","iopub.execute_input":"2025-01-20T01:27:29.156762Z","iopub.status.idle":"2025-01-20T01:27:29.167328Z","shell.execute_reply.started":"2025-01-20T01:27:29.156735Z","shell.execute_reply":"2025-01-20T01:27:29.166524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    tpd = []  # To collect probability differences for objects\n    tpd1 = []  # To collect probability differences for non-objects\n\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\n            inputs = inputs.to(device)\n            targets = [t.to(device) for t in targets]\n\n            outputs = model(inputs)\n            loss, conf_loss, box_loss, l2_loss = criterion(outputs, targets)\n            total_loss += loss.item()\n\n            # Iterate over each output-target pair\n            for output, target in zip(outputs, targets):\n                # Identify objects and non-objects\n                object_mask = target[..., 0] > 0\n                no_object_mask = target[..., 0] == 0\n\n                pred_probabilities = torch.sigmoid(output[..., 0])\n\n                # Collect probability differences for objects\n                if object_mask.any():\n                    predicted_probs_for_objects = pred_probabilities[object_mask]\n                    target_probs_for_objects = target[..., 0][object_mask]\n                    probability_diff = predicted_probs_for_objects - target_probs_for_objects\n                    tpd.extend(probability_diff.cpu().numpy())\n\n                # Collect probability differences for non-objects\n                if no_object_mask.any():\n                    predicted_probs_for_no_objects = pred_probabilities[no_object_mask]\n                    target_probs_for_no_objects = target[..., 0][no_object_mask]\n                    probability_diff1 = predicted_probs_for_no_objects - target_probs_for_no_objects\n                    tpd1.extend(probability_diff1.cpu().numpy())\n\n    # Compute average loss and probability differences\n    avg_loss = total_loss / len(dataloader)\n    avg_probability_diff = np.mean(np.abs(tpd)) if tpd else 0\n    avg_probability_diff1 = np.mean(np.abs(tpd1)) if tpd1 else 0\n\n    return avg_loss, avg_probability_diff, avg_probability_diff1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.168787Z","iopub.execute_input":"2025-01-20T01:27:29.169045Z","iopub.status.idle":"2025-01-20T01:27:29.180451Z","shell.execute_reply.started":"2025-01-20T01:27:29.169018Z","shell.execute_reply":"2025-01-20T01:27:29.179664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model(checkpoint_path, model_class, optimizer_class=None, scheduler_class=None, device=None):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    model = model_class()\n    chkpt = checkpoint['model_state_dict']\n    chkpt = {k.replace('module.', ''): v for k, v in chkpt.items()}\n    model.load_state_dict(chkpt)\n    model.to(device)\n    \n    if optimizer_class:\n        optimizer = optimizer_class(model.parameters())\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    else:\n        optimizer = None\n    \n    if scheduler_class and 'scheduler_state_dict' in checkpoint:\n        scheduler = scheduler_class(optimizer)\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    else:\n        scheduler = None\n    \n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    \n    return model, optimizer, scheduler, epoch, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:27:29.182034Z","iopub.execute_input":"2025-01-20T01:27:29.182281Z","iopub.status.idle":"2025-01-20T01:27:29.194012Z","shell.execute_reply.started":"2025-01-20T01:27:29.182255Z","shell.execute_reply":"2025-01-20T01:27:29.193312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YoloModel()\ncriterion = YoloLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2,min_lr=10**(-12))\n\nmodel_class = YoloModel\ncriterion = YoloLoss()\noptimizer_class = torch.optim.Adam\nscheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau\ntorch.backends.cudnn.benchmark = True\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ncheckpoint_path = '/kaggle/working/checkpoint.pth'\nmodel, optimizer, scheduler, start_epoch, loss = load_model(\n    checkpoint_path,\n    model_class,\n    optimizer_class,\n    scheduler_class,\n    device=device\n)\n\nfor g in optimizer.param_groups:\n    g['lr'] = 0.01\n\n\nmodel.to(device)\n\nscheduler.patience = 3\n\nearly_stopping = EarlyStopping(patience=5, verbose=True)\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    \n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    \n    \n    scheduler.step(train_loss)\n\n    val_loss, avg_prob, avg_prob_neg = validate_epoch(model, validation_loader, criterion, device)\n\n    current_lr = scheduler.get_last_lr()[0]\n\n    print(f'parameters: Epoch {epoch+1}/{num_epochs}, Average Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Probability: {avg_prob}, Negative Probability: {avg_prob_neg:.4f}, Learning Rate: {current_lr}')\n\n\n    early_stopping(val_loss)\n    if early_stopping.early_stop and current_lr <= 10**(-10):\n        print(\"Training stopped.\")\n        break","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-01-20T01:32:24.450223Z","iopub.execute_input":"2025-01-20T01:32:24.450899Z","iopub.status.idle":"2025-01-20T04:19:22.275811Z","shell.execute_reply.started":"2025-01-20T01:32:24.450867Z","shell.execute_reply":"2025-01-20T04:19:22.274938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef save_checkpoint(model, optimizer, scheduler, epoch, loss, filename):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'loss': loss,\n    }, filename)\n\n\nsave_checkpoint(model, optimizer, scheduler, epoch, train_loss, 'checkpoint.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:19:22.277211Z","iopub.execute_input":"2025-01-20T04:19:22.277489Z","iopub.status.idle":"2025-01-20T04:19:22.384337Z","shell.execute_reply.started":"2025-01-20T04:19:22.277462Z","shell.execute_reply":"2025-01-20T04:19:22.383458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_image(img, device):\n    \n    img = img.unsqueeze(0)\n    img = img.to(device)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.81068Z","iopub.status.idle":"2025-01-20T01:29:52.811134Z","shell.execute_reply.started":"2025-01-20T01:29:52.810907Z","shell.execute_reply":"2025-01-20T01:29:52.810929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_outputs(outputs, size):\n    \n    confidence = torch.sigmoid(outputs[0, ..., 0]).reshape(size, size)\n    positions = outputs[0, ..., 1:3].reshape(size, size, 2)\n    sizes = outputs[0, ..., 3:5].reshape(size, size, 2)\n\n    return [confidence, positions, sizes]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.812083Z","iopub.status.idle":"2025-01-20T01:29:52.8125Z","shell.execute_reply.started":"2025-01-20T01:29:52.812288Z","shell.execute_reply":"2025-01-20T01:29:52.81231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def non_max_suppression(boxes, iou_threshold):\n    if not boxes:\n        return []\n\n    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n    selected = []\n    while boxes:\n        max_box = boxes.pop(0)\n        selected.append(max_box)\n        boxes = [box for box in boxes if iou(max_box, box) < iou_threshold]\n    return selected\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.814081Z","iopub.status.idle":"2025-01-20T01:29:52.814497Z","shell.execute_reply.started":"2025-01-20T01:29:52.814281Z","shell.execute_reply":"2025-01-20T01:29:52.814304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou(box1, box2):\n    x1_min, y1_min, x1_max, y1_max, _ = box1\n    x2_min, y2_min, x2_max, y2_max, _ = box2\n\n    overlap_x_min = max(x1_min, x2_min)\n    overlap_y_min = max(y1_min, y2_min)\n    overlap_x_max = min(x1_max, x2_max)\n    overlap_y_max = min(y1_max, y2_max)\n\n    overlap_area = max(overlap_x_max - overlap_x_min, 0) * max(overlap_y_max - overlap_y_min, 0)\n    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n    iou = overlap_area / (area1 + area2 - overlap_area + 1e-6)\n    return iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.815648Z","iopub.status.idle":"2025-01-20T01:29:52.816086Z","shell.execute_reply.started":"2025-01-20T01:29:52.815868Z","shell.execute_reply":"2025-01-20T01:29:52.815889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_bounding_box(img, params, device):\n    \n    img = img.cpu()\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    img_denorm = img * std + mean\n\n    img_np = img_denorm.numpy()\n\n    img_np = (np.transpose(img_np[0], (1, 2, 0))).copy()\n\n    img_np = (img_np * 255).astype(np.uint8)\n\n    all_boxes = []\n    \n    for param in params:\n\n        confidence, positions, sizes, size = param\n        \n        for x in range(size):\n            for y in range(size):\n                if confidence[x, y] > 0.999:\n                    pos_x, pos_y = positions[x, y]\n    \n                    pos_x = pos_x.item()\n                    pos_y = pos_y.item()\n    \n                    width, height = sizes[x, y] \n    \n                    top_left = (int((x+1) * 416 / size - width / 2 + pos_x), (int((y+1) * 416 / size - height / 2)) + int(pos_y))\n                    bottom_right = (int((x+1) * 416 / size + width / 2 + pos_x), (int((y+1) * 416 / size + height / 2)) + int(pos_y))\n\n                    all_boxes.append((top_left[1], top_left[0], bottom_right[1], bottom_right[0], confidence[x, y]))\n                    \n    selected_boxes = non_max_suppression(all_boxes, iou_threshold=0.2)\n\n    for box in selected_boxes:\n        x_min, y_min, x_max, y_max, _ = box\n        \n        try:\n            img_np = img_np.astype(np.uint8)\n            img_np = cv2.rectangle(img_np, (x_min, y_min), (x_max, y_max), (255, 0, 0), 10)\n            \n        except Exception as e:\n            print(f\"Error drawing rectangle: {e}\")\n            continue\n\n    return img_np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.817537Z","iopub.status.idle":"2025-01-20T01:29:52.81805Z","shell.execute_reply.started":"2025-01-20T01:29:52.817805Z","shell.execute_reply":"2025-01-20T01:29:52.817829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def video_processing(model, path, sizes, transform = None, size = (416, 416), stream = cv2.COLOR_BGR2GRAY, device = 'cuda'):\n    \n    model.to(device)\n    model.eval()\n    cap = cv2.VideoCapture(path)\n\n    if not cap.isOpened():\n        \n        print(\"Ошибка при открытии видеофайла\")\n        \n\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    out = cv2.VideoWriter('/kaggle/working/output_video2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n    x = 0\n    while True:\n\n        ret, frame = cap.read()\n        \n    \n        if not ret:\n            break        \n        \n        frame = Image.fromarray(frame).convert('RGB')\n        resized_frame = frame.resize(size)\n        frame = frame.resize(size)\n\n        if transform != None:\n\n            frame = transform(frame).float()\n            frame_tensor = transform(resized_frame).float()\n\n        else: raise ValueError(\"processing is anavailable without data transformation\")\n        \n        frame = preprocess_image(frame,device)\n        frame_tensor = preprocess_image(frame_tensor, device)\n\n        with torch.no_grad():\n\n            outputs = model(frame_tensor)\n\n        param = [process_outputs(output, sizes[index])+[sizes[index]] for index,output in enumerate(outputs)]\n        img = draw_bounding_box(frame, param, device)\n\n        img = cv2.resize(img, (frame_width, frame_height))\n\n        if len(img.shape) == 2:\n\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n        img = img.astype(np.uint8)\n\n        # plt.imshow(img)\n        # plt.axis('on')\n        # plt.show()\n\n        out.write(img)\n\n    cap.release()\n    out.release()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.819676Z","iopub.status.idle":"2025-01-20T01:29:52.82011Z","shell.execute_reply.started":"2025-01-20T01:29:52.819891Z","shell.execute_reply":"2025-01-20T01:29:52.819914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_class = YoloModel\ncriterion = YoloLoss()\noptimizer_class = torch.optim.Adam\nscheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau\n\ncheckpoint_path = '/kaggle/working/checkpoint.pth'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel, _, _, _, _ = load_model(\n    checkpoint_path,\n    model_class,\n    optimizer_class = optimizer_class,\n    scheduler_class = scheduler_class,\n    device = device\n)\na = video_processing(model, '/kaggle/input/vi2fcd/download (5)', [13, 26, 52], transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T01:29:52.821186Z","iopub.status.idle":"2025-01-20T01:29:52.821453Z","shell.execute_reply.started":"2025-01-20T01:29:52.821321Z","shell.execute_reply":"2025-01-20T01:29:52.821334Z"}},"outputs":[],"execution_count":null}]}